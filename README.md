### InferLlama

This is a C++ inference engine for llama based architecture optimized to run on CPU.

