### InferLlama

This is a C/C++ inference engine for llama based architecture optimized to run on CPU.

